---
title: "An introduction to 'castarter' - content analysis starter toolkit for R"
author: "Giorgio Comai"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: how-to.bib
vignette: >
  %\VignetteIndexEntry{An introduction to 'castarter' - content analysis starter toolkit for R}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Introduction
Books dedicated to content analysis typically assume that the researcher has already created, has access or may buy access to a structured dataset.They may include sections on sampling [@krippendorff_content_2004; @riffe_analyzing_2005], but they do not debate explicitly how new datasets can be built. Commercial software packages generally share the same expectation. In the R ecosystem, there are a number of packages that can be used to access existing datasets (e.g. [tm.plugin.lexisnexis](https://cran.r-project.org/web/packages/tm.plugin.lexisnexis/), [manifestoR](https://cran.r-project.org/web/packages/manifestoR/)) or import into R textual content from social media through their APIs (e.g. [twitteR](https://cran.r-project.org/web/packages/twitteR/)). However, there is no package dedicated to getting into R the textual contents of regular websites and extracting key metadata (date and title) in order to populate a corpus that can be analysed in R or with other software packages. 'castarter - content analysis starter toolkit for R' aims to accomplish just that, offering to moderately incompetent users the opportunity to extract textual contents from websites and prepare them for content analysis. 

## Installation
`castarter` is hosted on GitHub. The easiest way to install it on your system, is to run the following commands in R:

```
install.packages("devtools")
library("devtools")
install_github("giocomai/castarter")
```
Load `castarter` with:
```
library("castarter")
```
While `castarter` is in early development, you may need to run the following function to load all dependencies. This function will eventually become obsolete and will be removed.
```{r}
library("castarter")
LoadRequiredPackages()
```
## Using castarter
`castarter` facilitates downloading and extracting textual contents from a given section of a website. `castarter` can be used to analyse one single website, or more websites as part of a project, facilitating comparison among cases. In order to smoothen replication and prevent data losses, by default `castarter` stores a copy of downloaded web pages in .html format, and of the extracted textual contents in .txt format (other export possibilities are available and presented below). To do so, it includes a function that creates the relevant folder structure.
First, you may want to set the root working folder, if you do not wish to use R's default, with:
```
setwd("/path/to/folder/")
```
Then, define name of project and name of website: 
```
nameOfProject <- "exampleProject"
nameOfWebsite <- "exampleWebsite"
CreateFolderStructure(nameOfProject, nameOfWebsite)
```
This creates the following folders

* exampleProject
    + exampleWebsite
        + Dataset
        + Html
        + IndexHtml
        + Logs
        + Outputs
        + Txt
        
In order to clarify the functioning of `castarter`, I will now show how to extract and prepare for content analysis all press releases currently available on the European Parliament's website. As will be shown, `castarter` takes advantage of the fact that most modern content management systems consistently generate URLs to present the contents of a website.
From the home page of the European Parliament, we can easily reach the main page dedicated to press releases clicking on 'News', and then on 'Press releases' on the top of the page: [http://www.europarl.europa.eu/news/en/news-room/press-release](http://www.europarl.europa.eu/news/en/news-room/press-release)
At the bottom of this page, as it is customary, the user is given the possibility to see older press released by clicking on the 'Next page' link. There is also the possibility to go back in time quicker, or seeing directly the oldest press release. Let's have a look at the URL's of these pages:
Clicking subsequent times on the 'next page' link, these are the URLs that we obtain:

http://www.europarl.europa.eu/news/en/news-room/press-release?start=10

http://www.europarl.europa.eu/news/en/news-room/press-release?start=20

http://www.europarl.europa.eu/news/en/news-room/press-release?start=30

Clicking on the link to the last available page, at the time of this writing, shows the following URLs, including links to press releases published in 2010:

http://www.europarl.europa.eu/news/en/news-room/press-release?start=4600

There is clearly a pattern: each page shows 10 press releases, and as we go back in time, the URLs "tells" the content management system to skip the first x number of press releases before starting. This makes it easy to create the links to these index pages. In `castarter`, this can be done by using the following function:
```
indexLinks <- CreateLinks(
linkFirstChunk = "http://www.europarl.europa.eu/news/en/news-room/press-release?start=",
startPage = 0,
endPage = 100,
increaseBy = 10)
```
This command creates a vector (`indexLinks`) including direct links to the index pages:
```{r}
library("castarter")
indexLinks <- CreateLinks(linkFirstChunk = "http://www.europarl.europa.eu/news/en/news-room/press-release?start=",
            startPage = 0,
            endPage = 100,
            increaseBy = 10)
print(indexLinks)
```

For this example, we will download only the first ten index pages, including links to 100 press releases. In order to download all of the press releases available, it is only necessary to change the `endPage` parameter, by changing the relevant parameter (at the time of this writing, this would be `endPage = 4600` instead of `endPage = 100`). 



## References