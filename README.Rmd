---
title: "An introduction to 'castarter' - content analysis starter toolkit for R"
author: "Giorgio Comai"
date: "`r Sys.Date()`"
output: github_document
bibliography: readme.bib
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "README-"
)
knitr::opts_knit$set(root.dir = normalizePath('./data'))
```

## Introduction
Books dedicated to content analysis typically assume that the researcher has already created, has access or may buy access to a structured dataset.They may include sections on sampling [@krippendorff_content_2004; @riffe_analyzing_2005], but they do not debate explicitly how new datasets can be built. Commercial software packages generally share the same expectation. In the R ecosystem, there are a number of packages that can be used to access existing datasets (e.g. [tm.plugin.lexisnexis](https://cran.r-project.org/web/packages/tm.plugin.lexisnexis/), [manifestoR](https://cran.r-project.org/web/packages/manifestoR/)) or import into R textual content from social media through their APIs (e.g. [twitteR](https://cran.r-project.org/web/packages/twitteR/)). However, there is no package dedicated to getting into R the textual contents of regular websites and extracting key metadata (date and title) in order to populate a corpus that can be analysed in R or with other software packages. 'castarter - content analysis starter toolkit for R' aims to accomplish just that, offering to moderately incompetent users the opportunity to extract textual contents from websites and prepare them for content analysis. 

## Installation
`castarter` is hosted on GitHub. The easiest way to install it on your system, is to run the following commands in R:

```
install.packages("devtools")
library("devtools")
install_github("giocomai/castarter")
```
Load `castarter` with:
```{r Load castarter, message=FALSE, warning=FALSE}
library("castarter")
```

## Using castarter
`castarter` facilitates downloading and extracting textual contents from a given section of a website. `castarter` can be used to analyse one single website, or more websites as part of a project, facilitating comparison among cases. In order to smoothen replication and prevent data losses, by default `castarter` stores a copy of downloaded web pages in .html format, and of the extracted textual contents in .txt format (other export possibilities are available and presented below). To do so, it includes a function that creates the relevant folder structure, and another function that facilitates archiving the .html files in compressed format.

In `castarter` every dataset is expected to be extracted from a website, and each website is expected to be part of a project. This is useful to give a consistent way of storing the materials gathered and to facilitate comparison among cases, but it does not imply additional limitations: it is possible to have projects of only one website, and it is possible to make comparisons among websites that are part of different projects. 

In order to explain the functioning of `castarter`, this document will now proceed to show how to extract the textual part of all press releases published on the current version of the European Parliament's website (©European Union, 2010-2016 – Source: European Parliament'). ^[Reproduction and use of contents published on the European Parliament's website is explicitly allowed by its [terms of use](http://www.europarl.europa.eu/portal/en/legal-notice).] As will be shown, `castarter` takes advantage of the fact that most modern content management systems consistently generate URLs and archive pages to present the contents of a website. This document shows only the most basic use of the package; additional parameters are explained elsewhere in the documentation or can be seen by running `?nameOfFunction` (still in development).

## Prepare the environment
First, define the name of project and the name of website. In this case, the project will be called "EuropeanUnion", assuming other websites to be analysed later may be of other EU institutions.
``` {r SetCastarter}
SetCastarter(nameOfProject = "EuropeanUnion", nameOfWebsite = "EuropeanParliament")
```
This stores nameOfProject and nameOfWebsite as options that can be retrieved by all `castarter` functions automatically, without the need to inputting them each time a function requires to save or load files. 

At this stage, one may consider setting the root working folder, if one does not wish to use R's default, with:
```{r Set folder, eval=FALSE, include=TRUE}
setwd("/path/to/folder/")
```
This step is of course optional.

Then, it is time to create the folder structure where all outputs will be saved:
```{r CreateFolderStructure, eval=FALSE, include=TRUE}
CreateFolderStructure()
```
This creates the following folders in the current working directory:

* EuropeanUnion
    + EuropeanParliament
        + Archives
        + Dataset
        + Html
        + IndexHtml
        + Logs
        + Outputs
        + Txt
        
Notice that there is no need to provide nameOfProject and nameOfWebsite to the function `CreateFolderStructure()`, since they are retrieved from the previously set options.

## Download index pages

From the home page of the European Parliament, it is easy to reach the main page dedicated to press releases clicking on 'News', and then on 'Press releases' on the top of the page: [http://www.europarl.europa.eu/news/en/news-room/press-release](http://www.europarl.europa.eu/news/en/news-room/press-release)
At the bottom of this page, as it is customary, the user is given the possibility to see older press released by clicking on the 'Next page' link. There is also the possibility to go back in time quicker, or seeing directly the oldest press release. Let's have a look at the URL's of these pages.
Clicking subsequent times on the 'next page' link, these are the URLs that we obtain:

http://www.europarl.europa.eu/news/en/news-room/press-release?start=10

http://www.europarl.europa.eu/news/en/news-room/press-release?start=20

http://www.europarl.europa.eu/news/en/news-room/press-release?start=30

Clicking on the link to the last available page, at the time of this writing, shows the following URLs, including links to press releases published in 2010:

http://www.europarl.europa.eu/news/en/news-room/press-release?start=5130

There is clearly a pattern: each page shows 10 press releases, and as we go back in time, the URLs "tells" the content management system to skip the first x number of press releases before starting. This makes it easy to create the links to these index pages. In `castarter`, this can be done by using the following function:
``` {r CreateLinks}
indexLinks <- CreateLinks(
    linkFirstChunk = "http://www.europarl.europa.eu/news/en/news-room/press-release?start=",
    startPage = 0,
    endPage = 5130,
    increaseBy = 10)
```
This command creates a vector (`indexLinks`) including direct links to the index pages. With the following command, it is possible to see the first links thus created:
```{r Show index links}
head(indexLinks)
```

It is now necessary to download these index pages. The following command downloads the relevant index pages and saves them in .html format in the /EuropeanUnion/EuropeanParliament/IndexHtml folder. 

```{r, eval=FALSE, include=TRUE}
DownloadContents(links = indexLinks, type = "index")
```
If the process is interrupted for any reason, it is possible simply to run again the same function, which will check which files have already been downloaded, and will download the missing ones.
Sometimes, due to a number of reasons, one or a few pages may not download correctly. It is usually appropriate to run the following function, which will re-download files that are too small to contain meaningful contents (if all pages have been downloaded correctly, the following will not do anything).
```{r, eval=FALSE, include=TRUE}
DownloadContents(links = indexLinks, type = "index", missingArticles = FALSE)
```
Then use the `ImportHtml` function to import these downloaded files in an R vector for further processing:
```{r, eval=FALSE, include=TRUE}
indexHtml <- ImportHtml(from = "index")
```

## Download all articles
Now that we have the index pages, we need to extract direct links to the individual press releases. There are various ways to do this, but the easiest approach is looking at the URLs of individual articles, and see if there is a pattern (this is very often the case in modern websites). Hovering over the titles of an index page, it is easy to see the links to indivudal news items. In this case, we see that the URL to each news item includes in the URL the following string: "/news/en/news-room/content/". We use this to extract relevant links, and discard all links to other pages or sections of the website scattered around the page that we are not interested in. The following command extracts links to individual articles, and tries to guess the title of the article.
```{r, eval=FALSE, include=TRUE}
articlesLinks <- ExtractLinks(domain = "http://www.europarl.europa.eu/",
                              partOfLink = "/news/en/news-room/",
                              indexHtml = indexHtml)
head(articlesLinks)
length(articlesLinks)
```
As expected, this function extracts more than 5000 links to individual pages. In this example, the links are correctly extracted and the titles match the links.

Now that we have direct links, it is easy to download all articles. The following command downloads all articles, stores them in a vector, saves them in .html format in the `/EuropeanUnion/EuropeanParliament/` folder, and notifies of advancement.
```{r, eval=FALSE, include=TRUE}
DownloadContents(links = articlesLinks, type = "articles")
```
As for the index files, it is advised to check if all files have been downloaded correctly, and then import them into R for further analysis. 
```{r, eval=FALSE, include=TRUE}
DownloadContents(links = indexLinks, type = "articles", missingArticles = FALSE)
indexHtml <- ImportHtml(from = "articles")
```

## Extracting metadata
It is now time to extract (or set) basic metadata from the articles: title, date, an ID, and language.
We have multiple options to extract the titles (use `?ExtractTitles` to find out more). For examples, titles can be extracted from individual pages, using the metadata embedded in the html page or by extracting the text that has style "heading 1".
```{r, eval=FALSE, include=TRUE}
titles <- ExtractTitles(articlesHtml = articlesHtml,
                        articlesLinks = articlesLinks,
                        titlesExtractMethod = "htmlTitle")

titles <- ExtractTitles(articlesHtml = articlesHtml,
                        articlesLinks = articlesLinks,
                        titlesExtractMethod = "htmlH1")
```
In this case, both methods seem to function reasonably well.

Extracting the date may be more complex, and `castarter` offers a number of different methods to extract dates. However, in many cases it is enough to provide the format of the date (more details are provided in the documentation of the function, accessible through the command `?ExtractDates` and `?ExtractDatesXpath`). In the case of the European Parliament, the date is presented in the format day-month-year, and it is extracted correctly using the default settings. 
```{r, eval=FALSE, include=TRUE}
dates <- ExtractDates(articlesHtml = articlesHtml,
                      dateFormat = "dmY")
```

There may be various criteria to set a unique ID for each article. The default option uses the same number given to the .html file stored in the  `/EuropeanUnion/EuropeanParliament/Html` folder.
```{r, eval=FALSE, include=TRUE}
articlesId <- ExtractArticleId()
```
The language can be set easily in subsequent steps, but storing it in a dedicated vector makes it easier to use template files.
```{r, eval=FALSE, include=FALSE}
language <- "en"
```
We can now store the metadata in a dedicated data.frame, and export them in a spreadsheet for future reference outside of R and `castarter`.
```{r, eval=FALSE, include=FALSE}
metadata <- ExportMetadata(nameOfProject = "EU",
                           nameOfWebsite = "EuropeanParliament",
                           dates = dates,
                           articlesId = articlesId,
                           titles = titles,
                           language = language,
                           articlesLinks = articlesLinks, 
                           exportXlsx = TRUE)
```

Metadata are also used to create relevant filenames for storing individual articles in textual format. The following command calls the `boilerpipeR` library and allows to extract only the main contents of each page, removing menus and other html clutter. 
```{r, eval=FALSE, include=FALSE}
articlesTxt <- ExtractTxt(articlesHtml, metadata)
```
Text extraction works well in most cases. However, at this stage it is generally a good idea to have a quick look at the extracted textual contents and remove easy-to-spot glitches. The following command prints on the terminal five random articles among all those extracted. 
```
articlesTxt[sample(1:length(articlesTxt), 5)]
```
At this stage, `castarter` has generated a folder of txt files and has exported relevant metadata. These can be imported and used in other software for quantitative or qualitative content analysis. The following command saves the current workspace in the working directory, stores the dataset in a dedicated folder for easy retrieval and allows to export both metadata and textual contents in a single spreadsheet. 

```{r, eval=FALSE, include=FALSE}
SaveAndExportWebsite(nameOfProject = "EU", 
                     nameOfWebsite = "EuropeanParliament", 
                     exportXlsx = TRUE)
```
At this stage, if we are satisified with the data we have, we may want to store all downloaded files in a .zip file and remove the original folders containing the original .html files. This way, we still can recover all of the original files, yet we don't have the thousands (or hundreds of thousands) of .html files that often are created in a `castarter` project, and may unnecessary slow down the backup process. This can easily be done with the following command.
```{r, eval=FALSE, include=FALSE}
ArchiveFolders(nameOfProject = "EU", 
               nameOfWebsite = "EuropeanParliament", 
               removeArchivedFolders = TRUE)
```
## Creating and polishing a corpus
For further analysis in `castarter`, it is useful to convert the data to a corpus of the `tm` package. Most of the functions offered by `castarter` are simply wrappers of `tm` functions. At this stage, it is possible to start a completely new R session, and simply load the dataset into the current workspace. All relevant information is effectively stored in the dataset. 
```{r, eval=FALSE, include=FALSE}
dataset <- LoadDatasets(projectsAndWebsites = "EU/EuropeanParliament")
corpus <- ConvertToCorpus(dataset)
```
The following  command allows to conduct a number of common operation on the corpus, such as transforming all text to lowercase, removing punctuation and removing numbers. These operations are enabled by default, but depending on the users needs can easily be disabled (e.g. by adding the paramater `removePunctuation = FALSE`)
```{r, eval=FALSE, include=FALSE}
corpus <- CleanCorpus(corpus)
```

When two or more separate words express a specific concept that is relevant to our research, we might want to combine those words and analyse them as one in following phases. For example, we might want to consider "European Union", and "EU" asone and the same word, "EuropeanUnion".
```{r, eval=FALSE, include=FALSE}
originalCombination <- "european union"
toBeUsed <- "EuropeanUnion"
wordCombinations <- AddWordCombinations(wordCombinations = NULL,
                                        originalCombination = originalCombination,
                                        toBeUsed = toBeUsed)
originalCombination <- "eu"
toBeUsed <- "EuropeanUnion"
wordCombinations <- AddWordCombinations(wordCombinations = wordCombinations,
                                        originalCombination = originalCombination,
                                        toBeUsed = toBeUsed)
wordCombinations
```
We can then apply these word combinations to the corpus. 
```{r, eval=FALSE, include=FALSE}
corpus <- CombineWords(corpus, wordCombinations)
```
It is common to remove stopwords, i.e. commonly used words that are substantially relevant to our research. The `tm` package provides a default list of stopwords, and we can add our own. However, such steps must be carefully supervisioned. "will" may be substantially irrelvant, unless we are interested, for example, in "political will"; "can" may be substantially irrelevant, unless, for example, we are interested in recycling, etc. 
```{r, eval=FALSE, include=FALSE}
stopwords <- c("will", "also", "can")
stopwords <- AddStopwords(newStopwords = stopwords,
                          nameOfProject = "EU", 
                          includeDefault = TRUE, 
                          language = "en")
corpus <- RemoveStopwords(corpus, stopwords)
```
A common next step is that of document stemming.
```{r, eval=FALSE, include=FALSE}
corpus <- tm::tm_map(corpus, tm::stemDocument)
```
Depending on the language and the desired result, word combinations may be better applied after stemming. 



## References